{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsy9fifYTJQKbBkLD13nmc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maschu09/mless/blob/main/remote_sensing/SATMAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration of SATMAE codebase\n",
        "\n",
        "In this tutorial, we will go through snippets of code and try to understand how much complicate it becomes with the development and experiments in real projects scenario."
      ],
      "metadata": {
        "id": "eqYBk_v_mS18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## There are several experiments conducted and discussed in [SATMAE](https://arxiv.org/abs/2207.08051) paper.\n",
        "\n",
        "Here, we will discuss only the experiments corresponding to SATMAE on fmow sentinel data with consistent masking. In the paper, it corresponds to SATMAE Group + CM\n",
        "\n",
        "Note: All the code snippets are from this [repository](https://github.com/sustainlab-group/SatMAE/tree/main)"
      ],
      "metadata": {
        "id": "G9YtaSOdnnUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learnings with previous notebook especially using CNN on SAT6 dataset\n",
        "\n",
        "- Preprocess data and create dataset and dataloader\n",
        "- Define the model which does the reconstruction task of the masked data\n",
        "- Evaluation of the model"
      ],
      "metadata": {
        "id": "fSQKkCFBqhs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Dataset in the pretraining script of SATMAE is discussed in this cell. The link to the script and where it is called  [link](https://github.com/sustainlab-group/SatMAE/blob/e31c11fa1bef6f9a9aa3eb49e8637c8b8952ba5e/main_pretrain.py#L136)\n",
        "\n",
        "```\n",
        "mean = SentinelIndividualImageDataset.mean\n",
        "std = SentinelIndividualImageDataset.std\n",
        "transform = SentinelIndividualImageDataset.build_transform(is_train, args.input_size, mean, std)\n",
        "dataset = SentinelIndividualImageDataset(csv_path, transform, masked_bands=args.masked_bands, dropped_bands=args.dropped_bands)\n",
        "```"
      ],
      "metadata": {
        "id": "Arg0VIQ0sPAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets look into the class SenintelIndividualImageDataset class\n",
        "\n",
        "```\n",
        "class SentinelIndividualImageDataset(SatelliteDataset):\n",
        "    label_types = ['value', 'one-hot']\n",
        "    mean = [1370.19151926, 1184.3824625 , 1120.77120066, 1136.26026392,\n",
        "            1263.73947144, 1645.40315151, 1846.87040806, 1762.59530783,\n",
        "            1972.62420416,  582.72633433,   14.77112979, 1732.16362238, 1247.91870117]\n",
        "    std = [633.15169573,  650.2842772 ,  712.12507725,  965.23119807,\n",
        "           948.9819932 , 1108.06650639, 1258.36394548, 1233.1492281 ,\n",
        "           1364.38688993,  472.37967789,   14.3114637 , 1310.36996126, 1087.6020813]\n",
        "\n",
        "    def __init__(self,\n",
        "                 csv_path: str,\n",
        "                 transform: Any,\n",
        "                 years: Optional[List[int]] = [*range(2000, 2021)],\n",
        "                 categories: Optional[List[str]] = None,\n",
        "                 label_type: str = 'value',\n",
        "                 masked_bands: Optional[List[int]] = None,\n",
        "                 dropped_bands: Optional[List[int]] = None):\n",
        "        \"\"\"\n",
        "        Creates dataset for multi-spectral single image classification.\n",
        "        Usually used for fMoW-Sentinel dataset.\n",
        "        :param csv_path: path to csv file.\n",
        "        :param transform: pytorch Transform for transforms and tensor conversion\n",
        "        :param years: List of years to take images from, None to not filter\n",
        "        :param categories: List of categories to take images from, None to not filter\n",
        "        :param label_type: 'values' for single label, 'one-hot' for one hot labels\n",
        "        :param masked_bands: List of indices corresponding to which bands to mask out\n",
        "        :param dropped_bands:  List of indices corresponding to which bands to drop from input image tensor\n",
        "        \"\"\"\n",
        "        super().__init__(in_c=13)\n",
        "        self.df = pd.read_csv(csv_path) \\\n",
        "            .sort_values(['category', 'location_id', 'timestamp'])\n",
        "\n",
        "        # Filter by category\n",
        "        self.categories = CATEGORIES\n",
        "        if categories is not None:\n",
        "            self.categories = categories\n",
        "            self.df = self.df.loc[categories]\n",
        "\n",
        "        # Filter by year\n",
        "        if years is not None:\n",
        "            self.df['year'] = [int(timestamp.split('-')[0]) for timestamp in self.df['timestamp']]\n",
        "            self.df = self.df[self.df['year'].isin(years)]\n",
        "\n",
        "        self.indices = self.df.index.unique().to_numpy()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        if label_type not in self.label_types:\n",
        "            raise ValueError(\n",
        "                f'FMOWDataset label_type {label_type} not allowed. Label_type must be one of the following:',\n",
        "                ', '.join(self.label_types))\n",
        "        self.label_type = label_type\n",
        "\n",
        "        self.masked_bands = masked_bands\n",
        "        self.dropped_bands = dropped_bands\n",
        "        if self.dropped_bands is not None:\n",
        "            self.in_c = self.in_c - len(dropped_bands)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def open_image(self, img_path):\n",
        "        with rasterio.open(img_path) as data:\n",
        "            # img = data.read(\n",
        "            #     out_shape=(data.count, self.resize, self.resize),\n",
        "            #     resampling=Resampling.bilinear\n",
        "            # )\n",
        "            img = data.read()  # (c, h, w)\n",
        "\n",
        "        return img.transpose(1, 2, 0).astype(np.float32)  # (h, w, c)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Gets image (x,y) pair given index in dataset.\n",
        "        :param idx: Index of (image, label) pair in dataset dataframe. (c, h, w)\n",
        "        :return: Torch Tensor image, and integer label as a tuple.\n",
        "        \"\"\"\n",
        "        selection = self.df.iloc[idx]\n",
        "\n",
        "        # images = [torch.FloatTensor(rasterio.open(img_path).read()) for img_path in image_paths]\n",
        "        images = self.open_image(selection['image_path'])  # (h, w, c)\n",
        "        if self.masked_bands is not None:\n",
        "            images[:, :, self.masked_bands] = np.array(self.mean)[self.masked_bands]\n",
        "\n",
        "        labels = self.categories.index(selection['category'])\n",
        "\n",
        "        img_as_tensor = self.transform(images)  # (c, h, w)\n",
        "        if self.dropped_bands is not None:\n",
        "            keep_idxs = [i for i in range(img_as_tensor.shape[0]) if i not in self.dropped_bands]\n",
        "            img_as_tensor = img_as_tensor[keep_idxs, :, :]\n",
        "\n",
        "        sample = {\n",
        "            'images': images,\n",
        "            'labels': labels,\n",
        "            'image_ids': selection['image_id'],\n",
        "            'timestamps': selection['timestamp']\n",
        "        }\n",
        "        return img_as_tensor, labels\n",
        "\n",
        "    @staticmethod\n",
        "    def build_transform(is_train, input_size, mean, std):\n",
        "        # train transform\n",
        "        interpol_mode = transforms.InterpolationMode.BICUBIC\n",
        "\n",
        "        t = []\n",
        "        if is_train:\n",
        "            t.append(SentinelNormalize(mean, std))  # use specific Sentinel normalization to avoid NaN\n",
        "            t.append(transforms.ToTensor())\n",
        "            t.append(\n",
        "                transforms.RandomResizedCrop(input_size, scale=(0.2, 1.0), interpolation=interpol_mode),  # 3 is bicubic\n",
        "            )\n",
        "            t.append(transforms.RandomHorizontalFlip())\n",
        "            return transforms.Compose(t)\n",
        "\n",
        "        # eval transform\n",
        "        if input_size <= 224:\n",
        "            crop_pct = 224 / 256\n",
        "        else:\n",
        "            crop_pct = 1.0\n",
        "        size = int(input_size / crop_pct)\n",
        "\n",
        "        t.append(SentinelNormalize(mean, std))\n",
        "        t.append(transforms.ToTensor())\n",
        "        t.append(\n",
        "            transforms.Resize(size, interpolation=interpol_mode),  # to maintain same ratio w.r.t. 224 images\n",
        "        )\n",
        "        t.append(transforms.CenterCrop(input_size))\n",
        "\n",
        "        return transforms.Compose(t)\n",
        "  ```"
      ],
      "metadata": {
        "id": "Av8DoT-nuAg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it is not clear from now but for self-supervised we only need the training dataset. We can also use the validation dataset just to test how well the model is learning but it is not mandatory as the application will not be used for inference setup."
      ],
      "metadata": {
        "id": "MHPEbHr7yntE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-training model\n",
        "\n",
        "Now the dataset is instantiated and next thing is to define the model which will take this data as an input and optimize for reconstruction loss\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from timm.models.vision_transformer import PatchEmbed, Block\n",
        "\n",
        "from util.pos_embed import get_2d_sincos_pos_embed, get_1d_sincos_pos_embed_from_grid\n",
        "\n",
        "\n",
        "class MaskedAutoencoderGroupChannelViT(nn.Module):\n",
        "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, spatial_mask=False,\n",
        "                 channel_groups=((0, 1, 2, 6), (3, 4, 5, 7), (8, 9)),\n",
        "                 channel_embed=256, embed_dim=1024, depth=24, num_heads=16,\n",
        "                 decoder_channel_embed=128, decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
        "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_c = in_chans\n",
        "        self.patch_size = patch_size\n",
        "        self.channel_groups = channel_groups\n",
        "        self.spatial_mask = spatial_mask  # Whether to mask all channels of same spatial location\n",
        "        num_groups = len(channel_groups)\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = nn.ModuleList([PatchEmbed(img_size, patch_size, len(group), embed_dim)\n",
        "                                          for group in channel_groups])\n",
        "        # self.patch_embed = PatchEmbed(img_size, patch_size, 1, embed_dim)\n",
        "        num_patches = self.patch_embed[0].num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim - channel_embed),\n",
        "                                      requires_grad=False)  # fixed sin-cos embedding\n",
        "        self.channel_embed = nn.Parameter(torch.zeros(1, num_groups, channel_embed), requires_grad=False)\n",
        "        # self.enc_mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "\n",
        "        self.decoder_pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, num_patches + 1, decoder_embed_dim - decoder_channel_embed),\n",
        "            requires_grad=False)  # fixed sin-cos embedding\n",
        "        # Extra channel for decoder to represent special place for cls token\n",
        "        self.decoder_channel_embed = nn.Parameter(torch.zeros(1, num_groups + 1, decoder_channel_embed),\n",
        "                                                  requires_grad=False)\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "\n",
        "        self.decoder_pred = nn.ModuleList([nn.Linear(decoder_embed_dim, len(group) * patch_size**2)\n",
        "                                           for group in channel_groups])\n",
        "        # self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size ** 2, bias=True)  # decoder to patch\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        self.norm_pix_loss = norm_pix_loss\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed[0].num_patches ** .5),\n",
        "                                            cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        channel_embed = get_1d_sincos_pos_embed_from_grid(self.channel_embed.shape[-1],\n",
        "                                                          torch.arange(len(self.channel_groups)).numpy())\n",
        "        self.channel_embed.data.copy_(torch.from_numpy(channel_embed).float().unsqueeze(0))\n",
        "\n",
        "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1],\n",
        "                                                    int(self.patch_embed[0].num_patches ** .5), cls_token=True)\n",
        "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        dec_channel_embed = get_1d_sincos_pos_embed_from_grid(self.decoder_channel_embed.shape[-1],\n",
        "                                                              torch.arange(len(self.channel_groups) + 1).numpy())\n",
        "        self.decoder_channel_embed.data.copy_(torch.from_numpy(dec_channel_embed).float().unsqueeze(0))\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        for patch_embed in self.patch_embed:\n",
        "            w = patch_embed.proj.weight.data\n",
        "            torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def patchify(self, imgs, p, c):\n",
        "        \"\"\"\n",
        "        imgs: (N, C, H, W)\n",
        "        p: Patch embed patch size\n",
        "        c: Num channels\n",
        "        x: (N, L, C*patch_size**2)\n",
        "        \"\"\"\n",
        "        # p = self.patch_embed.patch_size[0]\n",
        "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
        "\n",
        "        # c = self.in_c\n",
        "        h = w = imgs.shape[2] // p\n",
        "        x = imgs.reshape(shape=(imgs.shape[0], c, h, p, w, p))\n",
        "        x = torch.einsum('nchpwq->nhwcpq', x)\n",
        "        x = x.reshape(shape=(imgs.shape[0], h * w, p ** 2 * c))\n",
        "        return x\n",
        "\n",
        "    def unpatchify(self, x, p, c):\n",
        "        \"\"\"\n",
        "        x: (N, L, C*patch_size**2)\n",
        "        p: Patch embed patch size\n",
        "        c: Num channels\n",
        "        imgs: (N, C, H, W)\n",
        "        \"\"\"\n",
        "        # c = self.in_c\n",
        "        # p = self.patch_embed.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** .5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, c, p, p))\n",
        "        x = torch.einsum('nhwcpq->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        # x is (N, C, H, W)\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        x_c_embed = []\n",
        "        for i, group in enumerate(self.channel_groups):\n",
        "            x_c = x[:, group, :, :]\n",
        "            x_c_embed.append(self.patch_embed[i](x_c))  # (N, L, D)\n",
        "\n",
        "        x = torch.stack(x_c_embed, dim=1)  # (N, G, L, D)\n",
        "        _, G, L, D = x.shape\n",
        "\n",
        "        # add channel embed\n",
        "        channel_embed = self.channel_embed.unsqueeze(2)  # (1, G, 1, cD)\n",
        "        pos_embed = self.pos_embed[:, 1:, :].unsqueeze(1)  # (1, 1, L, pD)\n",
        "\n",
        "        # Channel embed same across (x,y) position, and pos embed same across channel (c)\n",
        "        channel_embed = channel_embed.expand(-1, -1, pos_embed.shape[2], -1)  # (1, G, L, cD)\n",
        "        pos_embed = pos_embed.expand(-1, channel_embed.shape[1], -1, -1)  # (1, G, L, pD)\n",
        "        pos_channel = torch.cat((pos_embed, channel_embed), dim=-1)  # (1, G, L, D)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + pos_channel  # (N, G, L, D)\n",
        "\n",
        "        if self.spatial_mask:\n",
        "            # Mask spatial location across all channels (i.e. spatial location as either all/no channels)\n",
        "            x = x.permute(0, 2, 1, 3).reshape(b, L, -1)  # (N, L, G*D)\n",
        "            x, mask, ids_restore = self.random_masking(x, mask_ratio)  # (N, 0.25*L, G*D)\n",
        "            x = x.view(b, x.shape[1], G, D).permute(0, 2, 1, 3).reshape(b, -1, D)  # (N, 0.25*G*L, D)\n",
        "            mask = mask.repeat(1, G)  # (N, G*L)\n",
        "            mask = mask.view(b, G, L)\n",
        "        else:\n",
        "            # Independently mask each channel (i.e. spatial location has subset of channels visible)\n",
        "            x, mask, ids_restore = self.random_masking(x.view(b, -1, D), mask_ratio)  # (N, 0.25*G*L, D)\n",
        "            mask = mask.view(b, G, L)\n",
        "\n",
        "        # append cls token\n",
        "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (N, G*L + 1, D)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore):\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x)  # (N, 1 + G*0.25*L, D)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        G = len(self.channel_groups)\n",
        "        if self.spatial_mask:\n",
        "            N, L = ids_restore.shape\n",
        "\n",
        "            x_ = x[:, 1:, :].view(N, G, -1, x.shape[2]).permute(0, 2, 1, 3)  # (N, 0.25*L, G, D)\n",
        "            _, ml, _, D = x_.shape\n",
        "            x_ = x_.reshape(N, ml, G * D)  # (N, 0.25*L, G*D)\n",
        "\n",
        "            mask_tokens = self.mask_token.repeat(N, L - ml, G)\n",
        "            x_ = torch.cat((x_, mask_tokens), dim=1)  # no cls token\n",
        "            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).expand(-1, -1, x_.shape[2]))  # (N, L, G*D)\n",
        "            x_ = x_.view(N, L, G, D).permute(0, 2, 1, 3).reshape(N, -1, D)  # (N, G*L, D)\n",
        "            x = torch.cat((x[:, :1, :], x_), dim=1)  # append cls token  (N, 1 + G*L, D)\n",
        "        else:\n",
        "            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "            x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "            x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token  (N, 1 + c*L, D)\n",
        "\n",
        "        # add pos and channel embed\n",
        "        channel_embed = self.decoder_channel_embed[:, :-1, :].unsqueeze(2)  # (1, G, 1, cD)\n",
        "        pos_embed = self.decoder_pos_embed[:, 1:, :].unsqueeze(1)  # (1, 1, L, pD)\n",
        "\n",
        "        channel_embed = channel_embed.expand(-1, -1, pos_embed.shape[2], -1)  # (1, G, L, cD)\n",
        "        pos_embed = pos_embed.expand(-1, channel_embed.shape[1], -1, -1)  # (1, G, L, pD)\n",
        "        pos_channel = torch.cat((pos_embed, channel_embed), dim=-1)  # (1, G, L, D)\n",
        "        pos_channel = pos_channel.view(1, -1, pos_channel.shape[-1])  # (1, G*L, D)\n",
        "\n",
        "        extra = torch.cat((self.decoder_pos_embed[:, :1, :],\n",
        "                           self.decoder_channel_embed[:, -1:, :]), dim=-1)  # (1, 1, D)\n",
        "\n",
        "        pos_channel = torch.cat((extra, pos_channel), dim=1)  # (1, 1+G*L, D)\n",
        "        x = x + pos_channel  # (N, 1+G*L, D)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        # Separate channel axis\n",
        "        N, GL, D = x.shape\n",
        "        x = x.view(N, G, GL//G, D)\n",
        "\n",
        "        # predictor projection\n",
        "        x_c_patch = []\n",
        "        for i, group in enumerate(self.channel_groups):\n",
        "            x_c = x[:, i]  # (N, L, D)\n",
        "            dec = self.decoder_pred[i](x_c)  # (N, L, g_c * p^2)\n",
        "            dec = dec.view(N, x_c.shape[1], -1, int(self.patch_size**2))  # (N, L, g_c, p^2)\n",
        "            dec = torch.einsum('nlcp->nclp', dec)  # (N, g_c, L, p^2)\n",
        "            x_c_patch.append(dec)\n",
        "\n",
        "        x = torch.cat(x_c_patch, dim=1)  # (N, c, L, p**2)\n",
        "        return x\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, c, H, W]\n",
        "        pred: [N, L, c*p*p]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        target = self.patchify(imgs, self.patch_embed[0].patch_size[0], self.in_c)  # (N, L, C*P*P)\n",
        "\n",
        "        if self.norm_pix_loss:\n",
        "            mean = target.mean(dim=-1, keepdim=True)\n",
        "            var = target.var(dim=-1, keepdim=True)\n",
        "            target = (target - mean) / (var + 1.e-6) ** .5\n",
        "\n",
        "        N, L, _ = target.shape\n",
        "        target = target.view(N, L, self.in_c, -1)  # (N, L, C, p^2)\n",
        "        target = torch.einsum('nlcp->nclp', target)  # (N, C, L, p^2)\n",
        "\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, C, L], mean loss per patch\n",
        "\n",
        "        total_loss, num_removed = 0., 0.\n",
        "        for i, group in enumerate(self.channel_groups):\n",
        "            group_loss = loss[:, group, :].mean(dim=1)  # (N, L)\n",
        "            total_loss += (group_loss * mask[:, i]).sum()\n",
        "            num_removed += mask[:, i].sum()  # mean loss on removed patches\n",
        "\n",
        "        return total_loss/num_removed\n",
        "\n",
        "    def forward(self, imgs, mask_ratio=0.75):\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, C, L, p*p]\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        return loss, pred, mask\n",
        "  ```"
      ],
      "metadata": {
        "id": "WejiW-Gu0jJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-training routine\n",
        "\n",
        "Now, that data and model is defined. Now both will be used for training routine with optimal hyperparameters\n",
        "\n",
        "```\n",
        "def train_one_epoch(model: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, loss_scaler,\n",
        "                    log_writer=None,\n",
        "                    args=None):\n",
        "    model.train(True)\n",
        "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 20\n",
        "\n",
        "    accum_iter = args.accum_iter\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if log_writer is not None:\n",
        "        print('log_dir: {}'.format(log_writer.log_dir))\n",
        "\n",
        "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "\n",
        "        # we use a per iteration (instead of per epoch) lr scheduler\n",
        "        if data_iter_step % accum_iter == 0:\n",
        "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
        "\n",
        "        samples = samples.to(device, non_blocking=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
        "\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            raise ValueError(f\"Loss is {loss_value}, stopping training\")\n",
        "            # sys.exit(1)\n",
        "\n",
        "        loss /= accum_iter\n",
        "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
        "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
        "        if (data_iter_step + 1) % accum_iter == 0:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        metric_logger.update(loss=loss_value)\n",
        "\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        metric_logger.update(lr=lr)\n",
        "\n",
        "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
        "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
        "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
        "            This calibrates different curves when batch size changes.\n",
        "            \"\"\"\n",
        "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
        "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
        "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
        "\n",
        "            # Wandb logging\n",
        "            if args.local_rank == 0 and args.wandb is not None:\n",
        "                try:\n",
        "                    wandb.log({'train_loss_step': loss_value_reduce,\n",
        "                               'train_lr_step': lr, 'epoch_1000x': epoch_1000x})\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "hwpVVJpxnFX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downstream Experiments\n",
        "\n",
        "Now that pretraining is done. Now the pre-trained model needs to be used for finetuning. Here, finetuning or transfer learning experiments will be demonstrated.\n",
        "\n",
        "Here, experiment setup for EuroSAT will be seen"
      ],
      "metadata": {
        "id": "8aaSdj-JoFNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pipeline\n",
        "\n",
        "```\n",
        "  mean, std = EuroSat.mean, EuroSat.std\n",
        "  transform = EuroSat.build_transform(is_train, args.input_size, mean, std)\n",
        "  dataset = EuroSat(csv_path, transform, masked_bands=args.masked_bands, dropped_bands=args.dropped_bands)\n",
        "```"
      ],
      "metadata": {
        "id": "ARofUGBBon52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class EuroSat(SatelliteDataset):\n",
        "    mean = [1370.19151926, 1184.3824625, 1120.77120066, 1136.26026392,\n",
        "            1263.73947144, 1645.40315151, 1846.87040806, 1762.59530783,\n",
        "            1972.62420416, 582.72633433, 14.77112979, 1732.16362238, 1247.91870117]\n",
        "    std = [633.15169573, 650.2842772, 712.12507725, 965.23119807,\n",
        "           948.9819932, 1108.06650639, 1258.36394548, 1233.1492281,\n",
        "           1364.38688993, 472.37967789, 14.3114637, 1310.36996126, 1087.6020813]\n",
        "\n",
        "    def __init__(self, file_path, transform, masked_bands=None, dropped_bands=None):\n",
        "        \"\"\"\n",
        "        Creates dataset for multi-spectral single image classification for EuroSAT.\n",
        "        :param file_path: path to txt file containing paths to image data for EuroSAT.\n",
        "        :param transform: pytorch Transform for transforms and tensor conversion\n",
        "        :param masked_bands: List of indices corresponding to which bands to mask out\n",
        "        :param dropped_bands:  List of indices corresponding to which bands to drop from input image tensor\n",
        "        \"\"\"\n",
        "        super().__init__(13)\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = f.read().splitlines()\n",
        "        self.img_paths = [row.split()[0] for row in data]\n",
        "        self.labels = [int(row.split()[1]) for row in data]\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        self.masked_bands = masked_bands\n",
        "        self.dropped_bands = dropped_bands\n",
        "        if self.dropped_bands is not None:\n",
        "            self.in_c = self.in_c - len(dropped_bands)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def open_image(self, img_path):\n",
        "        with rasterio.open(img_path) as data:\n",
        "            img = data.read()  # (c, h, w)\n",
        "\n",
        "        return img.transpose(1, 2, 0).astype(np.float32)  # (h, w, c)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.img_paths[idx], self.labels[idx]\n",
        "        img = self.open_image(img_path)  # (h, w, c)\n",
        "        if self.masked_bands is not None:\n",
        "            img[:, :, self.masked_bands] = np.array(self.mean)[self.masked_bands]\n",
        "\n",
        "        img_as_tensor = self.transform(img)  # (c, h, w)\n",
        "        if self.dropped_bands is not None:\n",
        "            keep_idxs = [i for i in range(img_as_tensor.shape[0]) if i not in self.dropped_bands]\n",
        "            img_as_tensor = img_as_tensor[keep_idxs, :, :]\n",
        "\n",
        "        return img_as_tensor, label\n",
        "```"
      ],
      "metadata": {
        "id": "cyRpSJE3pI8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model pipeline\n",
        "\n",
        "```\n",
        "class VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n",
        "    \"\"\" Vision Transformer with support for global average pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, global_pool=False, **kwargs):\n",
        "        super(VisionTransformer, self).__init__(**kwargs)\n",
        "\n",
        "        # Added by Samar, need default pos embedding\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches ** .5),\n",
        "                                            cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        self.global_pool = global_pool\n",
        "        if self.global_pool:\n",
        "            norm_layer = kwargs['norm_layer']\n",
        "            embed_dim = kwargs['embed_dim']\n",
        "            self.fc_norm = norm_layer(embed_dim)\n",
        "\n",
        "            del self.norm  # remove the original norm\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.global_pool:\n",
        "            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n",
        "            outcome = self.fc_norm(x)\n",
        "        else:\n",
        "            x = self.norm(x)\n",
        "            outcome = x[:, 0]\n",
        "\n",
        "        return outcome\n",
        "```"
      ],
      "metadata": {
        "id": "yA_K0hBeqUp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: It is important here to initialize this model with the weights of the pre-trained model\n",
        "\n",
        "```\n",
        "checkpoint = torch.load(args.finetune, map_location='cpu')\n",
        "\n",
        "print(\"Load pre-trained checkpoint from: %s\" % args.finetune)\n",
        "checkpoint_model = checkpoint['model']\n",
        "state_dict = model.state_dict()\n",
        "for k in ['pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'head.weight', 'head.bias']:\n",
        "      if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "            print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "            del checkpoint_model[k]\n",
        "\n",
        "# interpolate position embedding\n",
        "interpolate_pos_embed(model, checkpoint_model)\n",
        "\n",
        "# load pre-trained model\n",
        "model.load_state_dict(checkpoint_model, strict=False)\n",
        "```\n"
      ],
      "metadata": {
        "id": "h81CuzVMquzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downstream Finetuning\n",
        "\n",
        "Once the model is initialized, now training needs to be run with optimal hypeparameters\n",
        "\n",
        "```\n",
        "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
        "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device, epoch: int, loss_scaler, max_norm: float = 0,\n",
        "                    mixup_fn: Optional[Mixup] = None, log_writer=None,\n",
        "                    args=None):\n",
        "    model.train(True)\n",
        "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 20\n",
        "\n",
        "    accum_iter = args.accum_iter\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if log_writer is not None:\n",
        "        print('log_dir: {}'.format(log_writer.log_dir))\n",
        "\n",
        "    for data_iter_step, (samples, targets) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "\n",
        "        # we use a per iteration (instead of per epoch) lr scheduler\n",
        "        if data_iter_step % accum_iter == 0:\n",
        "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
        "\n",
        "        samples = samples.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        if mixup_fn is not None:\n",
        "            samples, targets = mixup_fn(samples, targets)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(samples)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            raise ValueError(f\"Loss is {loss_value}, stopping training\")\n",
        "\n",
        "        loss /= accum_iter\n",
        "        loss_scaler(loss, optimizer, clip_grad=max_norm,\n",
        "                    parameters=model.parameters(), create_graph=False,\n",
        "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
        "        if (data_iter_step + 1) % accum_iter == 0:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        metric_logger.update(loss=loss_value)\n",
        "        min_lr = 10.\n",
        "        max_lr = 0.\n",
        "        for group in optimizer.param_groups:\n",
        "            min_lr = min(min_lr, group[\"lr\"])\n",
        "            max_lr = max(max_lr, group[\"lr\"])\n",
        "\n",
        "        metric_logger.update(lr=max_lr)\n",
        "\n",
        "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
        "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
        "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
        "            This calibrates different curves when batch size changes.\n",
        "            \"\"\"\n",
        "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
        "            log_writer.add_scalar('loss', loss_value_reduce, epoch_1000x)\n",
        "            log_writer.add_scalar('lr', max_lr, epoch_1000x)\n",
        "\n",
        "            if args.local_rank == 0 and args.wandb is not None:\n",
        "                try:\n",
        "                    wandb.log({'train_loss_step': loss_value_reduce,\n",
        "                               'train_lr_step': max_lr, 'epoch_1000x': epoch_1000x})\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
        "```"
      ],
      "metadata": {
        "id": "2oP4n7bardJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSprrvCwthKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}