{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93fb8933",
   "metadata": {},
   "source": [
    "# 1. Understanding TOAR data\n",
    "\n",
    "[TOAR Data Infrastructure](https://toar-data.fz-juelich.de/) uses a REST API based data provisioning mechanism which is used to illustrate long range timeseries forecasting tasks using machine learning techniques on air quality data.\n",
    "\n",
    "Initially preliminary analysis is to be performed on the data and subsequently appropriate pre-processing approaches are determined to better forecast future values of the pollutant concentration.\n",
    "\n",
    "The task also iteratively introduces how performance and methodology has evolved systematically from classical ML techniques to recurrent neural networks in forecasting univariate timeseries. Illustrating the advantages and shorcomings in the approach.\n",
    "\n",
    "Finally to demostrate the state of the art capabilities of architectures like transformers which can be leveraged to do multi variate forecasting utilising known past meterological data.\n",
    "\n",
    "> Please note: In case of runtime loss or a need to run any segmented sections of the code make sure to run all the housekeeping cells before it\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13766b33",
   "metadata": {},
   "source": [
    "## Initial Setup and Data download\n",
    "> This section downloads example data from TOAR for 5 stations in Germany. Refer to [TOAR Quick UserGuide](https://toar-data.fz-juelich.de/sphinx/TOAR_UG_Vol02_Quick_Start/build/html/examples.html) examples to better understand data structuring in TOAR that is used in the below snippet to download examples.\n",
    "\n",
    "ðŸ˜ˆ **Question 1:** What are the potential challenges in working with observational environmental data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb9d93",
   "metadata": {},
   "source": [
    "### Housekeeping: Initial setup, declarations and method definitions\n",
    "\n",
    "ðŸ˜ˆ **Task 1:** Explore the `station_codes` variable and try changing the station(s) to a different region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb98c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.1.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.32.3)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (80.7.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sindhu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-win_amd64.whl (376.0 MB)\n",
      "Installing collected packages: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sindhu\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\client_channel\\\\lb_policy\\\\grpclb\\\\client_load_reporting_filter.h'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most notebook servers like google collab should have these packages pre-installed\n",
    "# In such cases this is just a sanity check\n",
    "!pip install pandas numpy requests tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4eba776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Global constants\n",
    "TIMESERIES_DATA_DIR = \"./content/timeseries_data/\"\n",
    "TIMESERIES_CSV_DIR = os.path.join(TIMESERIES_DATA_DIR, \"toar_csv_timeseries\")\n",
    "TIMESERIES_ID_FILE = os.path.join(TIMESERIES_DATA_DIR, \"timeseriesIDs.json\")\n",
    "MIN_FILE_SIZE_BYTES = 100\n",
    "group_columns = ['station_code', 'latitude', 'longitude']\n",
    "\n",
    "os.makedirs(TIMESERIES_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(TIMESERIES_CSV_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af26a1",
   "metadata": {},
   "source": [
    "Custom range selection for experiments\n",
    "\n",
    "station codes in the example snippet below are checked for a common range for the chosen variables, however if you prefer to experiment with ranges you might want to check using [Search API](https://toar-data.fz-juelich.de/api/v2/#search-combined-endpoint-of-stations-and-timeseries) for the date range for which data is available for a particular variable, station and source combination. As the TOAR is observational data provided via different contributing sources and has vast missing data.\n",
    "\n",
    "ðŸ˜ˆ **Question 2:** Why might there be gaps in observational data from TOAR?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9663fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# German stations with good distribution o3 variable observations\n",
    "station_codes = [\"DENW094\", \"DEBW073\",\"DEBB029\",\"DEBE051\",\"DEHE020\"]\n",
    "# station_codes = [\"DENW094\"]\n",
    "# variable_columns = [\"no2\", \"temp\", \"o3\", \"no\", \"press\"]\n",
    "variable_columns = [\"temp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d99569",
   "metadata": {},
   "source": [
    "Below methods each have appropriate documentation and comments to illustrate the logical flow *(they are placed with enough safeguards against both the API and optimized to avoid re-downloads when interupted during partial downloads to accomodate any loss of runtime on platforms like google colab)* and briefly described here for ease of use\n",
    ">\n",
    "\n",
    "ðŸ˜ˆ **Task 2:** Inspect the function `pivot_handle()`. What does it return, and why is pivoting important for time series analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef2eebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_timeseries_ids():\n",
    "    \"\"\"\n",
    "    Load existing timeseries IDs from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing stored timeseries metadata.\n",
    "    \"\"\"\n",
    "    return json.load(open(TIMESERIES_ID_FILE, 'r')) if os.path.exists(TIMESERIES_ID_FILE) else {}\n",
    "\n",
    "def save_timeseries_ids(timeseries_data):\n",
    "    \"\"\"\n",
    "    Save timeseries metadata to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        timeseries_data (dict): A dictionary containing timeseries metadata.\n",
    "    \"\"\"\n",
    "    json.dump(timeseries_data, open(TIMESERIES_ID_FILE, 'w'), indent=4)\n",
    "\n",
    "def fetch_timeseries_data(station_codes, existing_timeseries, variable_columns):\n",
    "    \"\"\"\n",
    "    Fetch timeseries metadata for given station codes, filtering by specified variables.\n",
    "\n",
    "    Args:\n",
    "        station_codes (list): List of station codes to fetch data for.\n",
    "        existing_timeseries (dict): Dictionary of previously fetched timeseries metadata.\n",
    "        variable_columns (list): List of variable names to retain.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary containing filtered timeseries metadata.\n",
    "    \"\"\"\n",
    "    base_url = \"http://toar-data.fz-juelich.de/api/v2/search/?codes=\"\n",
    "    unique_entries = existing_timeseries.copy()\n",
    "    processed_station_codes = {details['station_code'] for details in existing_timeseries.values()}\n",
    "\n",
    "    for code in station_codes:\n",
    "        if code in processed_station_codes:\n",
    "            print(f\"\\t\\tStation {code} is already processed, skipping.\")\n",
    "            continue\n",
    "\n",
    "        response = requests.get(base_url + code, timeout=1000)\n",
    "        if response.status_code == 200:\n",
    "            for entry in response.json():\n",
    "                if (variable_name := entry.get('variable', {}).get('name')) in variable_columns:\n",
    "                    timeseries_id = entry.get('id')\n",
    "                    if timeseries_id not in unique_entries:\n",
    "                        unique_entries[timeseries_id] = {\n",
    "                            'data_start_date': entry.get('data_start_date'),\n",
    "                            'data_end_date': entry.get('data_end_date'),\n",
    "                            'variable_name': variable_name,\n",
    "                            'station_code': code,\n",
    "                            'latitude': entry.get('station', {}).get('coordinates', {}).get('lat'),\n",
    "                            'longitude': entry.get('station', {}).get('coordinates', {}).get('lng'),\n",
    "                        }\n",
    "        else:\n",
    "            print(f\"\\t\\tFailed to fetch data for station {code}. Status code: {response.status_code}\")\n",
    "    return unique_entries\n",
    "\n",
    "def pivot_handle(dfs, metadata_columns, variable_columns):\n",
    "    \"\"\"\n",
    "    Pivot and structure the timeseries dataframe for sequential data analysis.\n",
    "\n",
    "    Args:\n",
    "        dfs (pd.DataFrame): Dataframe containing timeseries data.\n",
    "        metadata_columns (list): List of metadata column names.\n",
    "        variable_columns (list): List of variable names to include.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with pivoted structure.\n",
    "    \"\"\"\n",
    "    dfs = dfs[dfs['variable_name'].isin(variable_columns)]\n",
    "    pivot_df = dfs.pivot(index='datetime', columns='variable_name', values='value').reset_index()\n",
    "    return dfs[metadata_columns].drop_duplicates(subset=['datetime']).merge(pivot_df, on='datetime', how='left')\n",
    "\n",
    "def download_csv_data(timeseries_data, variable_columns):\n",
    "    \"\"\"\n",
    "    Download and process CSV data for each timeseries ID.\n",
    "\n",
    "    Args:\n",
    "        timeseries_data (dict): Dictionary containing timeseries metadata.\n",
    "        variable_columns (list): List of variable names to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataframe of all timeseries data.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    metadata_columns = ['datetime', 'station_code', 'latitude', 'longitude']\n",
    "\n",
    "    for ts_id, details in timeseries_data.items():\n",
    "        csv_path = os.path.join(TIMESERIES_CSV_DIR, f\"{ts_id}.csv\")\n",
    "\n",
    "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > MIN_FILE_SIZE_BYTES:\n",
    "            print(f\"\\tCSV already exists for timeseries ID {ts_id}, skipping download.\")\n",
    "        else:\n",
    "            print(f\"\\tDownloading data for timeseries ID {ts_id}\")\n",
    "            url = f\"http://toar-data.fz-juelich.de/api/v2/data/timeseries/{ts_id}?format=csv\"\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=1000)\n",
    "                response.raise_for_status()\n",
    "                with open(csv_path, 'wb') as file:\n",
    "                    file.writelines(response.iter_content(chunk_size=8192))\n",
    "                print(f\"\\t\\tRaw data CSV of {ts_id} saved: {csv_path}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\t\\tFailed to download data for timeseries ID {ts_id}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, skiprows=lambda i: i < next(i for i, line in enumerate(open(csv_path)) if line.startswith('datetime')), low_memory=False)\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')\n",
    "            df[['variable_name', 'station_code', 'latitude', 'longitude']] = details['variable_name'], details['station_code'], details['latitude'], details['longitude']\n",
    "            print(f\"Dataframe for timeseries ID {ts_id} loaded successfully with shape {df.shape}\")\n",
    "            dataframes.append(pivot_handle(df, metadata_columns, variable_columns))\n",
    "        except (pd.errors.EmptyDataError, pd.errors.ParserError) as e:\n",
    "            print(f\"\\tError processing CSV for timeseries ID {ts_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.concat(dataframes, ignore_index=True).sort_values(by=['station_code', 'datetime']) if dataframes else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab0925",
   "metadata": {},
   "source": [
    "### Download via REST API\n",
    "\n",
    "ðŸ˜ˆ **Task 3:** Try downloading a different variable or add another pollutant (e.g., `so2`). What changes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3464c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Number of time series meta data fetched : 3\n",
      "\tDownloading data for timeseries ID 76\n",
      "\t\tRaw data CSV of 76 saved: ./content/timeseries_data/toar_csv_timeseries\\76.csv\n",
      "Dataframe for timeseries ID 76 loaded successfully with shape (245580, 9)\n",
      "\tDownloading data for timeseries ID 22639\n",
      "\t\tRaw data CSV of 22639 saved: ./content/timeseries_data/toar_csv_timeseries\\22639.csv\n",
      "Dataframe for timeseries ID 22639 loaded successfully with shape (110890, 9)\n",
      "\tDownloading data for timeseries ID 18022\n",
      "\t\tRaw data CSV of 18022 saved: ./content/timeseries_data/toar_csv_timeseries\\18022.csv\n",
      "Dataframe for timeseries ID 18022 loaded successfully with shape (219901, 9)\n",
      "\t Total dataFrames processed : 576371 and shape of first dataframe (576371, 5).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>station_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245580</th>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245581</th>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245582</th>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245583</th>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245584</th>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        datetime station_code   latitude  longitude  temp\n",
       "245580 1997-01-01 00:00:00+00:00      DEBW073  47.819182   7.567796 -10.0\n",
       "245581 1997-01-01 01:00:00+00:00      DEBW073  47.819182   7.567796 -11.0\n",
       "245582 1997-01-01 02:00:00+00:00      DEBW073  47.819182   7.567796 -11.0\n",
       "245583 1997-01-01 03:00:00+00:00      DEBW073  47.819182   7.567796 -12.0\n",
       "245584 1997-01-01 04:00:00+00:00      DEBW073  47.819182   7.567796 -12.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load existing timeseries IDs from json to skip calls to TOAR\n",
    "existing_timeseries = load_existing_timeseries_ids()\n",
    "\n",
    "timeseries_data = fetch_timeseries_data(station_codes, existing_timeseries,variable_columns)\n",
    "print(f\"\\t Number of time series meta data fetched : {len(timeseries_data)}\")\n",
    "\n",
    "# save existing timeseries IDs as json to reduce calls to TOAR in future\n",
    "save_timeseries_ids(timeseries_data)\n",
    "\n",
    "dataframes = download_csv_data(timeseries_data,variable_columns)\n",
    "print(f\"\\t Total dataFrames processed : {len(dataframes)} and shape of first dataframe {dataframes.shape}.\")\n",
    "\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8d25f",
   "metadata": {},
   "source": [
    "As the TOAR is observational data provided via different contributing sources and has vast missing data, we expect the NAs despite finding a common date range and need to check how many missing values (NAs) there are for each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588a2f2",
   "metadata": {},
   "source": [
    "## Data handling (observational gaps)\n",
    "\n",
    "Once we know that, we can figure out the best way to fill in the gaps, like using interpolation or making the data more consistent. In our case as the data is towards pollutant concentration and data is hourly, we can safely fill a consecutive 6 hour window without compromising the quality of the data\n",
    "\n",
    "ðŸ˜ˆ **Question 3:** Why is it acceptable to fill up to 6 missing hourly values in this dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0237f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def fill_six_nans(group):\n",
    "    \"\"\"\n",
    "    Fills up to six consecutive NaN values in a given pandas Series using linear interpolation\n",
    "    if the NaNs are surrounded by valid values. If the NaNs are at the start, they are replaced\n",
    "    with zeros, and if they are at the end, they are filled with the last known value.\n",
    "\n",
    "    Args:\n",
    "        group (pd.Series): The input Series with potential NaN values.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series where up to six consecutive NaNs are interpolated, and longer NaN\n",
    "        sequences are partially filled while preserving the original index.\n",
    "    \"\"\"\n",
    "    values = group.to_numpy()\n",
    "    i = 0\n",
    "    while i < len(values):\n",
    "        if np.isnan(values[i]):\n",
    "            start = i\n",
    "            while i < len(values) and np.isnan(values[i]):\n",
    "                i += 1\n",
    "            end = min(i, start + 6)  # Limit to filling only 6 NaNs\n",
    "\n",
    "            if start > 0 and i < len(values):  # NaNs in the middle\n",
    "                fill_values = np.linspace(values[start - 1], values[i], end - start + 2)[1:-1]\n",
    "            elif start == 0:  # NaNs at the start\n",
    "                fill_values = [0] * (end - start)\n",
    "            elif i >= len(values):  # NaNs at the end\n",
    "                fill_values = [values[start - 1]] * (end - start)\n",
    "            values[start:end] = fill_values\n",
    "        else:\n",
    "            i += 1\n",
    "    return pd.Series(values, index=group.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d861e5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime        0\n",
       "station_code    0\n",
       "latitude        0\n",
       "longitude       0\n",
       "temp            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[variable_columns] = dataframes.groupby(group_columns)[variable_columns].transform(fill_six_nans)\n",
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c220f",
   "metadata": {},
   "source": [
    "Now rest of the Nas can be dropped as that station might not have data collected in the time period and the data needs to be normalized.\n",
    "\n",
    "ðŸ˜ˆ **Task 4:** What risks might arise if normalization is applied *before* handling missing values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079160de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime        0\n",
       "station_code    0\n",
       "latitude        0\n",
       "longitude       0\n",
       "temp            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = dataframes.dropna()\n",
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b6ef96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576371, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618005c",
   "metadata": {},
   "source": [
    "## Staged data loading (Housekeeping)\n",
    "\n",
    "To proceed with any analysis now the normalized data can be reloaded from local when needed (In case of runtime losses) to continue with ML experiments rather than re-downloading and normalizing again.\n",
    "\n",
    "ðŸ˜ˆ **Task 5:** Modify this section to reload data from a custom path or add a parameter to toggle reloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e51c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c18223",
   "metadata": {},
   "source": [
    "Below cell can be used to reload data if using an open source notebook servers like google colab and the if the usage limit is reached or for other issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bfaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>station_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime station_code   latitude  longitude  temp\n",
       "0  1997-01-01 00:00:00+00:00      DEBW073  47.819182   7.567796 -10.0\n",
       "1  1997-01-01 01:00:00+00:00      DEBW073  47.819182   7.567796 -11.0\n",
       "2  1997-01-01 02:00:00+00:00      DEBW073  47.819182   7.567796 -11.0\n",
       "3  1997-01-01 03:00:00+00:00      DEBW073  47.819182   7.567796 -12.0\n",
       "4  1997-01-01 04:00:00+00:00      DEBW073  47.819182   7.567796 -12.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## Normalized data csv is also made available for the select stations in URL:\n",
    "# https://drive.google.com/file/d/1cmTTWY3f18SikgRBcZzhtFswIf7XwPJq/view?usp=drive_link\n",
    "dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"))\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9d4b72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576371, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b4e968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime        0\n",
       "station_code    0\n",
       "latitude        0\n",
       "longitude       0\n",
       "temp            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426db96",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "The data can now be analysed for basic statistical tendencies or measures like (note standard measures are indicated here, but you can run custom measures as well)\n",
    "\n",
    "ðŸ˜ˆ **Task 6:** Create a boxplot for `no2` and `o3` by station to visualize spread and outliers.\n",
    "\n",
    "ðŸ˜ˆ **Question 4:** Which station shows the highest variance in `no2`? What might explain this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1205a5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"14\" halign=\"left\">temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "      <th>std</th>\n",
       "      <th>var</th>\n",
       "      <th>median</th>\n",
       "      <th>prod</th>\n",
       "      <th>nunique</th>\n",
       "      <th>5th_percentile</th>\n",
       "      <th>10th_percentile</th>\n",
       "      <th>25th_percentile</th>\n",
       "      <th>50th_percentile</th>\n",
       "      <th>75th_percentile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEBW073</th>\n",
       "      <td>-14.50</td>\n",
       "      <td>37.300000</td>\n",
       "      <td>11.009674</td>\n",
       "      <td>1.220863e+06</td>\n",
       "      <td>8.172790</td>\n",
       "      <td>66.794496</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>968</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEHE020</th>\n",
       "      <td>-18.20</td>\n",
       "      <td>38.842102</td>\n",
       "      <td>10.709302</td>\n",
       "      <td>2.354986e+06</td>\n",
       "      <td>8.084842</td>\n",
       "      <td>65.364675</td>\n",
       "      <td>10.24515</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>95966</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>0.643705</td>\n",
       "      <td>4.642115</td>\n",
       "      <td>10.24515</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>-17.95</td>\n",
       "      <td>39.126000</td>\n",
       "      <td>10.557351</td>\n",
       "      <td>2.592674e+06</td>\n",
       "      <td>7.280404</td>\n",
       "      <td>53.004277</td>\n",
       "      <td>10.28400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50737</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>1.265000</td>\n",
       "      <td>5.279750</td>\n",
       "      <td>10.28400</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               temp                                                           \\\n",
       "                min        max       mean           sum       std        var   \n",
       "station_code                                                                   \n",
       "DEBW073      -14.50  37.300000  11.009674  1.220863e+06  8.172790  66.794496   \n",
       "DEHE020      -18.20  38.842102  10.709302  2.354986e+06  8.084842  65.364675   \n",
       "DENW094      -17.95  39.126000  10.557351  2.592674e+06  7.280404  53.004277   \n",
       "\n",
       "                                                                    \\\n",
       "                median prod nunique 5th_percentile 10th_percentile   \n",
       "station_code                                                         \n",
       "DEBW073       11.00000 -0.0     968          -2.00        0.200000   \n",
       "DEHE020       10.24515 -0.0   95966          -1.40        0.643705   \n",
       "DENW094       10.28400  NaN   50737          -0.79        1.265000   \n",
       "\n",
       "                                                              \n",
       "             25th_percentile 50th_percentile 75th_percentile  \n",
       "station_code                                                  \n",
       "DEBW073             5.000000        11.00000            16.8  \n",
       "DEHE020             4.642115        10.24515            16.4  \n",
       "DENW094             5.279750        10.28400            15.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'min'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEBW073</th>\n",
       "      <td>-14.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEHE020</th>\n",
       "      <td>-18.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>-17.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               temp\n",
       "station_code       \n",
       "DEBW073      -14.50\n",
       "DEHE020      -18.20\n",
       "DENW094      -17.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'max'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEBW073</th>\n",
       "      <td>37.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEHE020</th>\n",
       "      <td>38.842102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>39.126000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   temp\n",
       "station_code           \n",
       "DEBW073       37.300000\n",
       "DEHE020       38.842102\n",
       "DENW094       39.126000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mean'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEBW073</th>\n",
       "      <td>11.009674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEHE020</th>\n",
       "      <td>10.709302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>10.557351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   temp\n",
       "station_code           \n",
       "DEBW073       11.009674\n",
       "DEHE020       10.709302\n",
       "DENW094       10.557351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'std'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DEBW073</th>\n",
       "      <td>8.172790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEHE020</th>\n",
       "      <td>8.084842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DENW094</th>\n",
       "      <td>7.280404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  temp\n",
       "station_code          \n",
       "DEBW073       8.172790\n",
       "DEHE020       8.084842\n",
       "DENW094       7.280404"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stats = ['min', 'max', 'mean', 'sum', 'std', 'var', 'median','prod','nunique',\n",
    "    ('5th_percentile', lambda x: x.quantile(0.05)),\n",
    "    ('10th_percentile', lambda x: x.quantile(0.10)),\n",
    "    ('25th_percentile', lambda x: x.quantile(0.25)),\n",
    "    ('50th_percentile', lambda x: x.quantile(0.50)), #(median)\n",
    "    ('75th_percentile', lambda x: x.quantile(0.75))]\n",
    "agg_dict = {col: stats for col in variable_columns}\n",
    "grouped = dataframes.groupby('station_code').agg(agg_dict)\n",
    "display(grouped)  \n",
    "\n",
    "for agg_func in ['min', 'max', 'mean', 'std']:\n",
    "    display(agg_func)\n",
    "    agg_view = grouped.xs(agg_func, axis=1, level=1) \n",
    "    display(agg_view)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1fc90",
   "metadata": {},
   "source": [
    "## Data Preprocessing (Normalization)\n",
    "\n",
    "the snippet below uses standard Z normalization (this is a simple snippet alternatively other approaches could also be used as desired)\n",
    "\n",
    "\n",
    "ðŸ˜ˆ **Task 7:** Implement min-max normalization and compare the results visually with Z-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8a947a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler(df, columns):\n",
    "    \"\"\"\n",
    "    Standardize the specified columns of a DataFrame by subtracting the mean\n",
    "    and dividing by the standard deviation (Z-score normalization).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized columns.\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    for col in columns:\n",
    "        mean = df_scaled[col].mean()\n",
    "        std = df_scaled[col].std()\n",
    "        df_scaled[col] = (df_scaled[col] - mean) / std\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff0651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>station_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.663793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.792464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.792464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.921135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.921135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime station_code   latitude  longitude      temp\n",
       "0  1997-01-01 00:00:00+00:00      DEBW073  47.819182   7.567796 -2.663793\n",
       "1  1997-01-01 01:00:00+00:00      DEBW073  47.819182   7.567796 -2.792464\n",
       "2  1997-01-01 02:00:00+00:00      DEBW073  47.819182   7.567796 -2.792464\n",
       "3  1997-01-01 03:00:00+00:00      DEBW073  47.819182   7.567796 -2.921135\n",
       "4  1997-01-01 04:00:00+00:00      DEBW073  47.819182   7.567796 -2.921135"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes = standard_scaler(dataframes, variable_columns)\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bad453c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a3eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>station_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-01 00:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.663793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-01 01:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.792464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-01-01 02:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.792464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-01-01 03:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.921135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-01-01 04:00:00+00:00</td>\n",
       "      <td>DEBW073</td>\n",
       "      <td>47.819182</td>\n",
       "      <td>7.567796</td>\n",
       "      <td>-2.921135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime station_code   latitude  longitude      temp\n",
       "0  1997-01-01 00:00:00+00:00      DEBW073  47.819182   7.567796 -2.663793\n",
       "1  1997-01-01 01:00:00+00:00      DEBW073  47.819182   7.567796 -2.792464\n",
       "2  1997-01-01 02:00:00+00:00      DEBW073  47.819182   7.567796 -2.792464\n",
       "3  1997-01-01 03:00:00+00:00      DEBW073  47.819182   7.567796 -2.921135\n",
       "4  1997-01-01 04:00:00+00:00      DEBW073  47.819182   7.567796 -2.921135"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## Normalized data csv is also made available for the select stations in URL:\n",
    "# https://drive.google.com/file/d/1Eui59GyTXNv839WijdZ0CkzuMubmZQz1/view?usp=drive_link \n",
    "# or if run locally you can load from the path as below:\n",
    "# e.g. r\"./content/timeseries_data/normalized_data.csv\"\n",
    "dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"))\n",
    "dataframes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efccd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
